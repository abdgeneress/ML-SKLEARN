{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA preprocessing\n",
    "\n",
    "#Imputation\n",
    "\"\"\"Imputation is simply the process of substituting the missing values of our dataset.We can do this by defining our own customised function or we can simply perform imputation by using the Imputer class provided by sklearn.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import imputation\n",
    "imputer = imputation(missing_values='NaN' ,strategy='mean')\n",
    "imputer = imputer.fit(df[['C','D']])\n",
    "df[['C','D']] = imputer.transform(df[['C','D']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardization\n",
    "\n",
    "\"\"\"It is another integral preprocessing step.In Standardization we transform our values such that the mean of the values is 0 and the standard deviation is 1.\"\"\"\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std = StandardScaler()\n",
    "X_train = std.fit_transform(X_train)\n",
    "X_test = std.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.55111512e-17 -3.70074342e-17  0.00000000e+00 -1.85037171e-17]\n",
      "[1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "#Mean removal\n",
    "\n",
    "\"\"\"It involves removing the mean from each feature so that it is centered on zero. Mean removal helps in removing any bias from the features.\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "input_data = np.array([[3, -1.5, 3, -6.4], [0, 3, -1.3, 4.1], [1, 2.3, -2.9, -4.3]])\n",
    "from sklearn import preprocessing\n",
    "\n",
    "datas = preprocessing.scale(input_data)\n",
    "print(datas.mean(axis=0))\n",
    "print(datas.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 1.        , 0.        ],\n",
       "       [0.        , 1.        , 0.27118644, 1.        ],\n",
       "       [0.33333333, 0.84444444, 0.        , 0.2       ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scaling\n",
    "\"\"\"The values of every feature in a data point can vary between random values. So, it is important to scale them so that this matches specified rules.\"\"\"\n",
    "data_scal = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "data_scaled = data_scal.fit_transform(input_data)\n",
    "data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.21582734, -0.10791367,  0.21582734, -0.46043165],\n",
       "       [ 0.        ,  0.35714286, -0.1547619 ,  0.48809524],\n",
       "       [ 0.0952381 ,  0.21904762, -0.27619048, -0.40952381]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalization\n",
    "\n",
    "\"\"\"Normalization involves adjusting the values in the feature vector so as to measure them on a common scale. Here, the values of a feature vector are adjusted so that they sum up to 1.\"\"\"\n",
    "dat_normalize = preprocessing.normalize(input_data, norm='l1')\n",
    "dat_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 1., 0.],\n",
       "       [0., 1., 0., 1.],\n",
       "       [0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Binarization\n",
    "\n",
    "\"\"\"Binarization is used to convert a numerical feature vector into a Boolean vector.\"\"\"\n",
    "data_binar = preprocessing.Binarizer(threshold=1.4)\n",
    "data_binrzd = data_binar.transform(input_data)\n",
    "data_binrzd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded vector = [[0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\envs\\intuitive-deep-learning\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#One Hot Encoding\n",
    "\n",
    "\"\"\"It may be required to deal with numerical values that are few and scattered, and you may not need to store these values.\"\"\"\n",
    "encoder = preprocessing.OneHotEncoder()\n",
    "encoder.fit([  [0, 2, 1, 12], \n",
    "               [1, 3, 5, 3], \n",
    "               [2, 3, 2, 12], \n",
    "               [1, 2, 4, 3]\n",
    "])\n",
    "encoded_vector = encoder.transform([[2, 3, 5, 3]]).toarray()\n",
    "print( \"\\nEncoded vector =\", encoded_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bmw --> 0\n",
      "ford --> 1\n",
      "suzuki --> 2\n",
      "toyota --> 3\n"
     ]
    }
   ],
   "source": [
    "#Label Encoding\n",
    "\n",
    "\"\"\"we mostly come across a variety of labels which can be in the form of numbers or words.\"\"\"\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "input_classes = ['suzuki', 'ford', 'suzuki', 'toyota', 'ford', 'bmw']\n",
    "label_encoder.fit(input_classes)\n",
    "for i, item in enumerate(label_encoder.classes_):\n",
    "    print (item, '-->', i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regresion\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#error\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,mean_squared_log_error, median_absolute_error\n",
    "\n",
    "#train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#datasets\n",
    "from sklearn import datasets\n",
    "\n",
    "#logistic regression\n",
    "from sklearn.linear_model importrt LogisticRegression\n",
    "\n",
    "#classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Decission tree clasifir and regressor\n",
    "from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\n",
    "\n",
    "#random forest\n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor,RandomTreesEmbedding\n",
    "\n",
    "#naive_bayes\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB,\n",
    "\n",
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "\n",
    "#SVM \n",
    "from sklearn.svm import SVC,SVR\n",
    "\n",
    "#un-supervised learning\n",
    "#k-means clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#PCA\n",
    "from sklearn.decomposition import PCA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
